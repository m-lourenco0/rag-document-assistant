{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33b7d9e8-03ef-4160-b916-a64ddd4e7a8b",
   "metadata": {},
   "source": [
    "# RAG System Evaluation\n",
    "\n",
    "## Evaluation Methodology\n",
    "\n",
    "This document outlines the methodology used to evaluate the performance of our Retrieval-Augmented Generation (RAG) system. Our goal is to quantitatively measure the quality and reliability of the generated answers.\n",
    "\n",
    "### Golden Dataset Creation\n",
    "\n",
    "To establish a ground truth for our evaluation, a \"golden dataset\" of question-and-answer pairs was created. This process was done manually by uploading the four source PDFs directly into the Gemini chat interface. For each document, a series of diverse and relevant questions were asked, and the resulting answers and verbatim source references were carefully collected. This hands-on approach, followed by a human review, ensures the dataset is high-quality and accurately reflects the content of the source material.\n",
    "\n",
    "### Evaluation Approach\n",
    "\n",
    "Our evaluation uses an **\"LLM-as-a-Judge\"** framework. The process is orchestrated following these steps:\n",
    "\n",
    "1.  **Querying**: Each question from the golden dataset is sent to our RAG system's API to retrieve its generated answer and source references.\n",
    "2.  **Judging**: A powerful, impartial LLM is then used as a judge to compare our RAG system's output against the ground truth from the golden dataset.\n",
    "3.  **Scoring**: The judge scores each response on two key metrics:\n",
    "    * **Faithfulness**: Assesses if the generated answer is strictly supported by the retrieved context, penalizing any hallucinations.\n",
    "    * **Answer Relevance**: Measures how well the answer addresses the user's original question.\n",
    "\n",
    "The aggregated scores from this process provide a quantitative measure of our RAG system's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb014f64-3d03-499a-a647-69d693bad29e",
   "metadata": {},
   "source": [
    "### 1. Setup & Imports\n",
    "\n",
    "First, we'll set up the notebook's environment. This block imports all the necessary Python libraries for file handling (`os`, `json`), data manipulation (`pandas`), and making API calls (`requests`). We also define key configuration variables, like the API endpoint and file paths, in one central place for easy modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29681eac-5b1a-4aca-8c35-a61ddd12343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- Configuration ---\n",
    "RAG_API_URL = \"http://localhost:8000/api/question\"\n",
    "DATASET_DIR = \"evaluation/datasets\"\n",
    "RESULTS_FILE = \"evaluation/evaluation_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f20e8c-e7d1-4790-9184-b50731461242",
   "metadata": {},
   "source": [
    "### 2. Configure the LLM-as-a-Judge\n",
    "\n",
    "This block sets up the core logic for our evaluation: the **\"LLM-as-a-Judge.\"** This automated evaluator will score our RAG system's performance consistently.\n",
    "\n",
    "First, we define a `JudgeScores` schema using Pydantic. This forces the LLM to provide its feedback in a clean, predictable JSON format, which is crucial for collecting and analyzing the results. We then initialize a powerful model (`gpt-4o`) to act as the judge, with its `temperature` set to `0.0` for maximum objectivity.\n",
    "\n",
    "Finally, we create a detailed prompt that serves as the instruction set for our judge, defining the key metrics: **Faithfulness** (is the answer based on the sources?) and **Answer Relevance** (does it answer the question?). These components are combined into a single `evaluation_chain` that is now ready to receive data and return a structured score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76415fd8-40f8-49a9-a369-054f6e0105d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeScores(BaseModel):\n",
    "    \"\"\"A Pydantic model to define the structured output for the judge LLM.\"\"\"\n",
    "    faithfulness_score: float = Field(\n",
    "        description=\"The score for faithfulness (0.0 for hallucination, 1.0 for fully faithful).\"\n",
    "    )\n",
    "    relevance_score: float = Field(\n",
    "        description=\"The score for relevance (0.0 for irrelevant, 0.5 for partial, 1.0 for fully relevant).\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"A brief justification for the assigned scores.\"\n",
    "    )\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.0)\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an impartial judge evaluating the quality of a RAG system's response.\n",
    "    Your task is to score the system's \"Faithfulness\" and \"Answer Relevance\".\n",
    "    - Faithfulness: Does the answer strictly rely on the provided context? Score 1 if yes, 0 if no (hallucination).\n",
    "    - Answer Relevance: Does the answer directly and completely address the user's question? Score 1 for a complete answer, 0.5 for a partial answer, and 0 for a non-relevant answer.\n",
    "\n",
    "    Provide your response as a valid JSON object with \"faithfulness_score\", \"relevance_score\", and a brief \"reason\".\n",
    "\n",
    "    ---\n",
    "    User Question: \"{question}\"\n",
    "    ---\n",
    "    Retrieved Context (for Faithfulness check): \"{rag_references}\"\n",
    "    ---\n",
    "    RAG System Answer: \"{rag_answer}\"\n",
    "    ---\n",
    "    Ground Truth Answer (for Relevance check): \"{ground_truth_answer}\"\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "evaluation_chain = prompt_template | llm.with_structured_output(JudgeScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a8a39-3643-4a1a-93ec-51b39b182dee",
   "metadata": {},
   "source": [
    "### 3. Helper Function: Query RAG API\n",
    "\n",
    "This helper function serves as the direct interface between our evaluation notebook and the live RAG application. Its purpose is to take a single `question`, send it to the RAG API, and return the system's response.\n",
    "\n",
    "A new, unique `thread_id` is generated for every call, ensuring each question is treated as an independent, single-turn query without any conversational history. The function includes a crucial `time.sleep(10)` delay to respect the API rate limits of the underlying services (like the Cohere reranker). Finally, it's wrapped in robust error handling to prevent the entire evaluation from crashing if a single API call fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68bf44-61ce-46d8-9e9c-2eb7c389ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag_api(question: str) -> dict:\n",
    "    \"\"\"Sends a question to the RAG API and returns the response.\"\"\"\n",
    "    payload = {\"question\": question, \"thread_id\": str(uuid.uuid4())}\n",
    "    try:\n",
    "        # Add time sleep to comply with free API key rate limit from Coherence\n",
    "        time.sleep(10)\n",
    "        response = requests.post(RAG_API_URL, json=payload, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"API call failed for question '{question}': {e}\")\n",
    "        return {\"answer\": None, \"references\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca658e18-f0d0-42fc-9f4b-dcde93b52b1f",
   "metadata": {},
   "source": [
    "### 4. Helper Function: LLM Judge\n",
    "\n",
    "This is the final and most critical helper function, which executes the **\"LLM-as-a-Judge\"** logic.\n",
    "\n",
    "It takes all the relevant information for a single data point—the question, the RAG system's answer, its retrieved references, and the ground truth answer—and passes them to the `evaluation_chain` we configured earlier. The function then returns a dictionary containing the judge's scores for **Faithfulness** and **Relevance**, along with a brief reason for its decision. Crucially, the entire call is wrapped in robust error handling to ensure that our evaluation loop can continue running even if a specific API call to the judge model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b8bb7-89e4-4579-ba99-d4194df847f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_judge_score(question: str, rag_answer: str, rag_references: list, ground_truth_answer: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses a LangChain-powered LLM to judge the RAG system's output.\n",
    "    \"\"\"\n",
    "    # Ensure the list of references is formatted as a clean string for the prompt\n",
    "    references_str = json.dumps(rag_references, indent=2)\n",
    "\n",
    "    try:\n",
    "        # Invoke the chain with the required inputs\n",
    "        result_obj = evaluation_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"rag_answer\": rag_answer,\n",
    "            \"rag_references\": references_str,\n",
    "            \"ground_truth_answer\": ground_truth_answer\n",
    "        })\n",
    "        # The result is a Pydantic object, so we convert it to a dictionary\n",
    "        return result_obj.model_dump()\n",
    "    except Exception as e:\n",
    "        # Handle potential API errors or parsing failures\n",
    "        print(f\"An error occurred during LLM judging for question '{question}': {e}\")\n",
    "        return {\n",
    "            \"faithfulness_score\": None,\n",
    "            \"relevance_score\": None,\n",
    "            \"reason\": f\"Error: {e}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2af4569-fa10-4b1c-80c7-920ecc82e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing results from evaluation/evaluation_results.csv...\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store all evaluation results\n",
    "if os.path.exists(RESULTS_FILE):\n",
    "    print(f\"Loading existing results from {RESULTS_FILE}...\")\n",
    "    all_results_df = pd.read_csv(RESULTS_FILE)\n",
    "else:\n",
    "    print(\"Initializing a new DataFrame for evaluation results.\")\n",
    "    all_results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb658381-26a3-4dfb-8008-c266636279f9",
   "metadata": {},
   "source": [
    "### 5. Main Evaluation Function\n",
    "\n",
    "This is the central function that runs a complete evaluation for a single PDF. It takes the filename of a PDF as input and performs a three-step process:\n",
    "\n",
    "1.  **Load Data**: It first finds and loads the corresponding \"golden\" JSON dataset.\n",
    "2.  **Query RAG**: It then iterates through every question in that dataset, calling the `query_rag_api` function to get the live response from your RAG application.\n",
    "3.  **Judge Results**: Finally, it sends the question, the RAG's response, and the ground truth answer to the `get_llm_judge_score` function to get a performance score.\n",
    "\n",
    "The function returns a complete Pandas DataFrame containing all this information, which can then be saved and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd850b4f-1825-443b-883d-b0e06a893fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_dir: str, pdf_filename: str):\n",
    "    \"\"\"\n",
    "    Loads a golden dataset using a string path, queries the RAG system, \n",
    "    and returns the judged results as a DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"--- Starting evaluation for {pdf_filename} ---\")\n",
    "    \n",
    "    # Construct the path using simple string joining\n",
    "    dataset_filename = pdf_filename.replace('.pdf', '.json')\n",
    "    dataset_path = f\"{dataset_dir}/{dataset_filename}\"\n",
    "    \n",
    "    try:\n",
    "        # Load dataset and add source document key\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            records = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find dataset file at path: {dataset_path}\")\n",
    "        return None\n",
    "        \n",
    "    batch_df = pd.DataFrame(records)\n",
    "    batch_df['source_document'] = pdf_filename\n",
    "    \n",
    "    # Query RAG API for each question\n",
    "    tqdm.pandas(desc=f\"Querying for {pdf_filename}\")\n",
    "    rag_results = batch_df[\"question\"].progress_apply(query_rag_api)\n",
    "    batch_df[\"rag_answer\"] = [r.get(\"answer\") for r in rag_results]\n",
    "    batch_df[\"rag_references\"] = [r.get(\"references\") for r in rag_results]\n",
    "\n",
    "    # Use LLM as a Judge\n",
    "    eval_scores = []\n",
    "    for index, row in tqdm(batch_df.iterrows(), total=len(batch_df), desc=f\"Judging for {pdf_filename}\"):\n",
    "        scores = get_llm_judge_score(\n",
    "            row[\"question\"],\n",
    "            row[\"rag_answer\"],\n",
    "            row[\"rag_references\"],\n",
    "            row[\"ground_truth_answer\"]\n",
    "        )\n",
    "        eval_scores.append(scores)\n",
    "    \n",
    "    scores_df = pd.DataFrame(eval_scores)\n",
    "    final_batch_df = pd.concat([batch_df, scores_df], axis=1)\n",
    "    \n",
    "    print(f\"--- Finished evaluation for {pdf_filename} ---\\n\")\n",
    "    return final_batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1fa9dd-cb55-4967-b9ed-6710768158a4",
   "metadata": {},
   "source": [
    "### 6. Execute Evaluation for a Specific PDF\n",
    "\n",
    "This is the main action cell for our manual workflow. After we have restarted your RAG application and uploaded a specific PDF, we run this cell to kick off the evaluation for that document.\n",
    "\n",
    "It calls our main `evaluate_dataset` function with the specific filename we want to evaluate. If the evaluation is successful, it appends the results to our master `all_results_df` DataFrame and immediately saves the progress to the CSV file. This ensures that even if a later step fails, the results from this run are safely stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd8ac9a2-8769-4b14-a1a8-1deb1be69d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting evaluation for LB5001.pdf ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bbb1e5d3a84efb89ba4da73644154e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Querying for LB5001.pdf:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1122b1b1b6c440f863c50fe16e5b618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging for LB5001.pdf:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished evaluation for LB5001.pdf ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf1_results = evaluate_dataset(dataset_dir=DATASET_DIR, pdf_filename=\"LB5001.pdf\")\n",
    "if pdf1_results is not None:\n",
    "    all_results_df = pd.concat([all_results_df, pdf1_results], ignore_index=True)\n",
    "    all_results_df.to_csv(RESULTS_FILE, index=False) # Save progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca7e06c-e7c7-4ceb-9010-cb39b3b1c1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting evaluation for MN414_0224.pdf ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067b4cb7a84440b68494c583f34fd2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Querying for MN414_0224.pdf:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aa4ae0968a4e4fbcc8a291321adf57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging for MN414_0224.pdf:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished evaluation for MN414_0224.pdf ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf2_results = evaluate_dataset(dataset_dir=DATASET_DIR, pdf_filename=\"MN414_0224.pdf\")\n",
    "if pdf2_results is not None:\n",
    "    all_results_df = pd.concat([all_results_df, pdf2_results], ignore_index=True)\n",
    "    all_results_df.to_csv(RESULTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e16168b0-f7c1-497c-be5f-3c1a3ce20843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting evaluation for WEG-CESTARI-manual-iom-guia-consulta-rapida-50111652-pt-en-es-web.pdf ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1a58b98ed041c0a7391b4e215757f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Querying for WEG-CESTARI-manual-iom-guia-consulta-rapida-50111652-pt-en-es-web.pdf:   0%|          | 0/7 [00:0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f805789064647b49236d205129c64c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging for WEG-CESTARI-manual-iom-guia-consulta-rapida-50111652-pt-en-es-web.pdf:   0%|          | 0/7 [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished evaluation for WEG-CESTARI-manual-iom-guia-consulta-rapida-50111652-pt-en-es-web.pdf ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf3_results = evaluate_dataset(dataset_dir=DATASET_DIR, pdf_filename=\"WEG-CESTARI-manual-iom-guia-consulta-rapida-50111652-pt-en-es-web.pdf\")\n",
    "if pdf3_results is not None:\n",
    "    all_results_df = pd.concat([all_results_df, pdf3_results], ignore_index=True)\n",
    "    all_results_df.to_csv(RESULTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "556128c9-bc5a-404a-9511-801392e516b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting evaluation for WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5a78b5f46e46d98f092c6309264d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Querying for WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf:   0%|          …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54aa07fb64774f3b83f887940db9a966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Judging for WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf:   0%|          |…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished evaluation for WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdf4_results = evaluate_dataset(dataset_dir=DATASET_DIR, pdf_filename=\"WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf\")\n",
    "if pdf4_results is not None:\n",
    "    all_results_df = pd.concat([all_results_df, pdf4_results], ignore_index=True)\n",
    "    all_results_df.to_csv(RESULTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac556200-19a8-464a-ae53-19148ddc69c7",
   "metadata": {},
   "source": [
    "### 7. Final Analysis and Results\n",
    "\n",
    "This is the final step of our evaluation process. After running the evaluation for all individual documents, this block calculates and displays the aggregate results. It provides a high-level summary of the RAG system's performance by calculating the overall average scores for **Faithfulness** and **Relevance** across all questions. It also computes a per-document breakdown, allowing us to see how the system performed on each specific document's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c2a8941-6aa1-4530-a40e-7b0371ec69cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Overall Evaluation Results ---\n",
      "Total questions evaluated: 28\n",
      "\n",
      "Overall Average Faithfulness Score: 1.00\n",
      "Overall Average Answer Relevance Score: 0.84\n",
      "\n",
      "--- Scores by Document ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_document</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LB5001.pdf</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MN414_0224.pdf</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEG-CESTARI-manual-iom-guia-consulta-rapida-50111652-pt-en-es-web.pdf</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEG-motores-eletricos-guia-de-especificacao-50032749-brochure-portuguese-web.pdf</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    faithfulness_score  \\\n",
       "source_document                                                          \n",
       "LB5001.pdf                                                         1.0   \n",
       "MN414_0224.pdf                                                     1.0   \n",
       "WEG-CESTARI-manual-iom-guia-consulta-rapida-501...                 1.0   \n",
       "WEG-motores-eletricos-guia-de-especificacao-500...                 1.0   \n",
       "\n",
       "                                                    relevance_score  \n",
       "source_document                                                      \n",
       "LB5001.pdf                                                 0.928571  \n",
       "MN414_0224.pdf                                             0.857143  \n",
       "WEG-CESTARI-manual-iom-guia-consulta-rapida-501...         0.857143  \n",
       "WEG-motores-eletricos-guia-de-especificacao-500...         0.714286  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"--- Overall Evaluation Results ---\")\n",
    "print(f\"Total questions evaluated: {len(all_results_df)}\")\n",
    "\n",
    "# Overall average scores\n",
    "avg_faithfulness = all_results_df[\"faithfulness_score\"].mean()\n",
    "avg_relevance = all_results_df[\"relevance_score\"].mean()\n",
    "\n",
    "print(f\"\\nOverall Average Faithfulness Score: {avg_faithfulness:.2f}\")\n",
    "print(f\"Overall Average Answer Relevance Score: {avg_relevance:.2f}\")\n",
    "\n",
    "# Per-document average scores\n",
    "print(\"\\n--- Scores by Document ---\")\n",
    "per_doc_scores = all_results_df.groupby('source_document')[['faithfulness_score', 'relevance_score']].mean()\n",
    "per_doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1517cb-daea-4b89-8b7b-947ae31c70c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
