# RAG System Evaluation Report

This document outlines the methodology and results of a quantitative evaluation performed on the RAG system.

-----

## Evaluation Methodology

Our goal is to measure the quality and reliability of the answers generated by the RAG system. The evaluation follows an **"LLM-as-a-Judge"** framework.

### 1\. Golden Dataset Creation

To establish a ground truth, a "golden dataset" of 28 question-and-answer pairs was created. This process was done manually by uploading the four source PDFs into the Gemini chat interface. For each document, a series of diverse and relevant questions were asked, and the resulting answers and verbatim source references were carefully collected and curated. This ensures the dataset is high-quality and accurately reflects the source material.

### 2\. Automated Evaluation

The evaluation was orchestrated in a Jupyter Notebook (`rag_evaluation.ipynb`) and followed these steps:

1. **Isolated Context**: For each of the four documents, the RAG application was restarted to ensure a clean state, and only the single relevant PDF was uploaded and indexed.
2. **Querying**: Each question from the corresponding golden dataset was sent to the RAG system's API to retrieve its generated answer and source references.
3. **Judging**: A powerful, impartial LLM (`gpt-4o`) was then used as a judge to compare the RAG system's output against the ground truth from the golden dataset.
4. **Scoring**: The judge scored each response on two key metrics:
      * **Faithfulness**: Assesses if the generated answer is strictly supported by the retrieved context, penalizing any hallucinations.
      * **Answer Relevance**: Measures how well the answer directly and completely addresses the user's original question.

-----

## Executive Summary

The evaluation demonstrates that the RAG system is highly reliable and effective. It achieved a perfect **Faithfulness** score, indicating it does not invent information. The **Answer Relevance** is strong, though there is room for improvement in the completeness of some answers.

### Overall Performance

| Metric | Score |
| :--- | :---: |
| **Answer Faithfulness** | **1.00** |
| **Answer Relevance** | **0.84** |

### Performance by Document

| Source Document | Faithfulness Score | Relevance Score |
| :--- | :---: | :---: |
| `LB5001.pdf` | 1.00 | 0.93 |
| `MN414_0224.pdf` | 1.00 | 0.86 |
| `WEG-CESTARI-manual...` | 1.00 | 0.86 |
| `WEG-motores-eletricos-guia...`| 1.00 | 0.71 |

-----

## Analysis and Key Findings

* **Faithfulness (1.00/1.00)**: The perfect score in Faithfulness is the most significant result. It confirms that the RAG system is **highly reliable** and successfully grounds its answers in the retrieved context. The retrieval and generation components work effectively together to prevent hallucinations.

* **Answer Relevance (0.84/1.00)**: A score of 0.84 indicates that the system's answers are **consistently on-topic and correct**. The gap from a perfect score suggests that while the answers are good, they are sometimes not as comprehensive or detailed as the ground truth. This variance is visible in the per-document scores, with the system performing best on the more straightforward `LB5001.pdf` manual.

-----

## Example Cases

Below are two examples from the evaluation that illustrate the system's performance.

### âœ… Success Case

* **Question**: "What are the primary components of a three-phase induction motor's stator and rotor as described in the guide?"
* **RAG System Answer**: "A three-phase induction motor is primarily composed of two main parts: the stator and the rotor. The stator is the fixed part and contains the three-phase winding, which is responsible for creating the rotating magnetic field when connected to an AC power source. The rotor is the moving part and is mounted on the shaft. It typically consists of a squirrel-cage type winding with bars and short-circuit rings, or a wound rotor with windings connected to slip rings."
* **Scores**:
  * **Faithfulness**: 1.0
  * **Relevance**: 1.0
* **Analysis**: A perfect score. The RAG system correctly identified all the key components mentioned in the source and synthesized them into a complete and accurate answer.

### A Case for Improvement

* **Question**: "Summarize the characteristics and typical applications for motor categories N, H, and D."
* **RAG System Answer**: "Motor categories N, H, and D are defined by their torque and starting current characteristics. Category N motors have normal starting torque and are suitable for general-purpose applications like pumps and fans. Category H motors have high starting torque, making them suitable for loads that are difficult to start, such as conveyors and crushers. Category D motors have very high starting torque and high slip, used for applications like elevators and presses."
* **Scores**:
  * **Faithfulness**: 1.0
  * **Relevance**: 0.5
* **Analysis**: The answer is factually correct and faithful to the source. However, the judge assigned a lower relevance score because the ground truth answer in the golden dataset contained more specific details about starting current values (e.g., "starting current is generally less than 6.5 times the rated current") which the RAG's summary omitted. The answer is good, but not as comprehensive as it could be.

-----

## Conclusion and Next Steps

The evaluation confirms that the RAG system is a robust and reliable tool for querying technical documents. Its perfect faithfulness score inspires high confidence in its outputs.

The primary area for improvement is in the **completeness and detail of the generated answers**. To improve the Answer Relevance score, future work could focus on:

* **Prompt Engineering**: Refining the final prompt sent to the LLM to encourage more comprehensive, detailed answers.
* **Chunking Strategy**: Experimenting with different chunk sizes or overlap to see if providing slightly more context to the LLM improves answer depth.
* **Retriever Tuning**: For documents that scored lower, analyzing the retrieved chunks to see if a different retrieval strategy could provide more relevant context for the LLM to synthesize.

For full transparency, the complete raw data from this evaluation is available in the repository on `notebooks/evaluation/evaluation_results.csv`.
